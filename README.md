This repository contains analysis code for the project '_Overnight circuit remodelling drives juvenile alloparental care_' by Bradley B. Jamieson, Maxwell X. Chen, Swang Liang, Lina S. H. El Rasheed, Patty Wai, Grace M. K. Chattey, Maria Voronkov, Emma Davis, J. Mark Skehel, I. Lorena Arancibia-Cárcamo, Molly Strom and Johannes Kohl

**ANOVA_vsAll**
This script performs all-by-all pairwise statistical comparisons between experimental groups in a CSV (specifically columns whose names start with “P”, e.g., P14, P14.5, P15). It loads the file, coerces entries to numeric, reshapes the data to long format, and then assesses normality across groups using Shapiro–Wilk (for groups with at least 3 values). If the data are deemed normal, it runs an OLS ANOVA for each group pair and reports Holm-corrected pairwise t-tests; if not, it runs Kruskal–Wallis for each pair followed by Dunn’s post-hoc test with Holm correction. Results are collated into a table including the groups compared, test used, raw and corrected p-values, and significance stars, and are saved as a *_results.csv file alongside the input.

**ANOVA_vsControl**
This script compares a single user-defined control group against every other experimental group in a CSV (intended for columns like P14, P15, …). It loads the dataset, coerces values to numeric, reshapes the data into long format, and tests normality with Shapiro–Wilk (for groups with ≥3 values) to decide whether to use parametric or non-parametric statistics. For each comparison (Control vs one other group), it runs an OLS-based ANOVA when the data are considered normal, then applies a Dunnett-style multiple-comparisons correction via Pingouin for the control-vs-group post-hoc p-value; if the data are not normal, it uses a Kruskal–Wallis test followed by Dunn’s post-hoc test with Holm correction. The script outputs a results table containing the tests used, raw and corrected p-values, and significance stars, and saves it as *_results.csv.

**BrainGlobe_Processing**
This script loads summary.csv file containing per-structure cell counts and volumes, then removes a predefined list of excluded brain regions (mostly fibre tracts/ventricles/large parent structures) based on the structure_name column. For the remaining structures it computes a cell density metric (Cells per mm^3) by dividing total_cells by total_volume_mm3. It then groups the data by structure name and produces a summarized table (summing total_cells and the computed density column across entries for each structure), and finally exports the organised summary to an Excel file  and prints the result.

**Chi2_Independence**
This script reads a contingency table from a CSV (first column used as row labels), coerces all entries to numeric, and drops any rows containing missing values. It expects exactly two groups (i.e., two columns), and then runs a Pearson chi-square test of independence (chi2_contingency) on the full table (described in the output as a 2×3 comparison—two groups across three categories, assuming three rows). The resulting p-value is then adjusted using Benjamini–Hochberg FDR correction (even though only a single p-value is being corrected), and the script outputs a one-row results table including the raw and corrected p-values plus significance stars. Finally, it saves the results to a new CSV named *_chi2_results.csv and prints the table to screen.

**Discrimination**
This script loads a CSV of z-scored responses where each column is a stimulus, converts it to long format, and then evaluates stimulus selectivity in two ways: (1) for each stimulus it computes a one-vs-rest ROC AUC using z-score as the classifier score and estimates the mean ± SEM AUC via 1000 bootstrap resamples (skipping resamples with only one class), and (2) it uses those bootstrapped AUC distributions to build a pairwise “dominance” matrix giving P(AUC_A > AUC_B) and a summary of how often each stimulus strongly beats others. It then computes a tuning specificity index (standardised mean difference of that stimulus vs all others using pooled SD), bootstraps its distribution, tests whether it is reliably > 0 with a one-sided Wilcoxon test, applies Benjamini–Hochberg FDR correction across stimuli, assigns significance stars, also calculates a per-animal per-stimulus TSI from each row of the original wide table, and finally exports all summaries (AUC ranks, dominance stats, pairwise matrix, tuning stats, and per-animal TSI) to a single results CSV.

**Elbow_points**
This script loads all spine-statistics CSV files from a specified directory, drops missing values, removes the first column (assumed to be an ID), and keeps only a user-specified set of feature columns for clustering. It concatenates all files into one dataset, standardizes features using StandardScaler, and then fits a series of K-means models across a range of cluster numbers (K=1–10), recording the inertia for each. To automatically estimate the “best” K, it computes the second derivative of the inertia curve and selects the K at the strongest change (the elbow). Finally, it plots the elbow curve with a vertical line marking the estimated elbow point and prints the chosen K value.

**Fisher's_multiple**
This script loads a CSV containing 2×N contingency data where rows are labelled “In” and “Out” (used as the index) and columns represent experimental groups/conditions. After coercing values to numeric and dropping missing data, it performs Fisher’s Exact Test on a series of adjacent column pairs (column 1 vs 2, 3 vs 4, etc.), building a 2×2 table for each comparison using the In/Out counts from the two columns. It then applies Benjamini–Hochberg FDR correction across all the pairwise Fisher p-values, assigns significance stars based on the corrected p-values, and saves a summary results table (raw p, corrected p, test name, and group labels) to an output CSV.

**Fisher's_vsControl**
This script loads a CSV containing In/Out counts (with the first column used as row labels, and the index expected to include “In” and “Out”). After converting entries to numeric and dropping missing values, it checks that a user-specified control column exists (assumed to be the last column in the table). It then iterates through every other column and, for each one, builds a 2×2 contingency table comparing that group’s In/Out counts to the control’s In/Out counts and runs Fisher’s Exact Test. For each comparison it reports the raw p-value, applies a Benjamini–Hochberg FDR correction (as written, this correction is applied per-test rather than across the full set of comparisons), assigns significance stars based on the corrected p-value, and saves a summary table to a results CSV.

**Gene_ANOVA**
This script loads a gene-specific CSV and reshapes it into long format assuming each column name encodes Region and Age as Region_Age (splitting on the last underscore). It then fits a two-way ANOVA model (Value ~ Region * Age) to test for main effects of Region and Age and their interaction, saving the ANOVA table to disk. For post-hoc testing, it performs pairwise age comparisons within each region using the pooled within-group variance (MS_within) and residual degrees of freedom from the ANOVA. It computes t-statistics and two-sided p-values for every age pair within each region, then applies Benjamini–Hochberg FDR correction across all of these within-region tests, adds q-values and significance stars, and exports the multiple-comparisons results to a separate CSV.

**KaplanMeier_multiple**
This script loads a CSV where each column is a timepoint/group of retrieval values, reshapes it into long format, and defines an event as retrieval < 600 (with ≥600 treated as censored). It then runs Kaplan–Meier log-rank tests (via lifelines) for adjacent column pairs in the input table (column 1 vs 2, 3 vs 4, etc.), storing the raw p-value and test details for each comparison. After all pairwise tests are run, it applies a Benjamini–Hochberg FDR correction across the full set of raw p-values, updates the results table with corrected p-values and significance stars, and saves the output as *_results.csv.

**KaplanMeier_vsControl**
This script loads a CSV where each column is a group/timepoint (with Vir. treated as the control), reshapes it into long format (one observation per row), and defines a binary event as whether retrieval time is < 600 (event occurred) versus ≥600 (treated as censored). It then runs a Kaplan–Meier log-rank test (via lifelines) comparing Vir. vs each other timepoint in turn, stores the raw p-values, and applies a Benjamini–Hochberg FDR correction across all control-vs-timepoint comparisons. Finally, it writes a results table (raw and corrected p-values plus significance stars) to *_results.csv.

**Linear_regression**
This script loads a CSV and extracts two numeric columns as the predictor (X) and response (Y) (skipping the first data row). It fits a simple ordinary least squares (OLS) linear regression with an intercept using statsmodels, then reports the fitted slope, intercept, R², and the p-value for the slope term. Using the fitted model, it computes predicted values and residuals, estimates the residual standard deviation, and draws an approximate 90% confidence band around the fitted line (±1.645 × residual SD). It visualises the raw data, regression line, and shaded 90% band in a scatter plot, and saves a small results CSV containing the regression equation, R², and p-value.

**MassSpec_CellChatDB**
This R script sets a working directory, loads the CellChat mouse ligand–receptor database, and then imports a hypothalamus-specific differential expression table (P15 vs P14). It filters that DE table to significant, strongly changing proteins (FDR < 0.05 and |log2FC| > 1) and uses the resulting protein list to subset CellChat’s interaction table, keeping only interactions where the ligand symbol matches the hypothalamus-enriched proteins. In the second section, it loads a full expression matrix (all_regions.csv) and matching sample metadata (all_regions_meta.csv), reshapes each target gene (Gas6, Sema4a, Sema5a, Ptprd) into long format, joins metadata, and creates an ordered region × age factor (CTX/HYP/STR across P10/P14/P15). For each gene it then generates a boxplot with jittered sample points across the region–age groups and saves each plot as a separate PDF in the figures/ folder.

**MassSpec_PCA_DEs**
This script loads normalised protein intensities (all_regions.csv) and sample metadata (all_regions_meta.csv), then collapses duplicate protein names by keeping the entry with the highest mean abundance across samples. It performs a PCA on hypothalamus (HYP) samples only (after removing rows with non-finite values), plots PC1 vs PC2 coloured by age with sample labels and 68% ellipses, and saves the figure as figures/PCA.pdf. For differential expression it fits a two-factor limma model with a region × age interaction (~ 0 + region*age), defines contrasts for P15 vs P14 within each region (CTX/HYP/STR), then derives “region-specific” contrasts (e.g., HYP_specific = HYP(P15vsP14) − mean(CTX,STR)) and exports the resulting region-specific DE protein tables (e.g., data/P15vP14_HYP_SPEC_DEs.csv). Finally, it builds a hypothalamus-specific volcano-plot dataframe (−log10 FDR vs logFC), merges in WGCNA module assignments, auto-selects labels (top hits by FDR and effect size plus a few named proteins), and saves two volcano PDFs: one coloured by Up in P14 / Up in P15 / N.S., and another coloured by WGCNA module colour (with non-significant points grey).

**MassSpec_Run_WGCNA**
This script loads normalised protein intensities (data/all_regions.csv) and sample metadata, then collapses duplicate protein names by keeping the row with the highest mean abundance across samples. It converts zeros to NA, filters to proteins with sufficient data coverage (≥70% finite values), aligns the expression matrix to the metadata, and runs a signed WGCNA network (bicor correlation) after choosing a soft-threshold power (set here to 21). Module assignments are written out as data/WGCNA_gene_module_assignments.csv. It then computes module eigengenes, builds an age×region trait design matrix, correlates modules with traits using bicor, applies BH correction to the module–trait p-values, exports the underlying correlation/p-value tables, and saves a labelled module–trait heatmap to figures/WGCNA_heatmap.pdf. Finally, it reads per-module GO enrichment tables from data/WGCNA/*_top_GO.csv, computes log2 fold-enrichment and FDR “stars”, generates a barplot for each module coloured by module colour, arranges them in a grid, and exports figures/WGCNA_GO_barplots_3x3.pdf.

**MassSpec_Run_fGSEA**
This script loads a mouse GO gene-set collection from a GMT file and a hypothalamus-specific differential expression table (P15vP14_HYP_SPEC_DEs.csv). It builds a ranked list using log2FC (sorted high to low, with protein IDs as names) and runs multilevel FGSEA (fgseaMultilevel) across the GO pathways (minSize=5, maxSize=5000). The enrichment results (pathway, adjusted p-value, NES, and set size) are saved to data/P15vP14_HYP_SPEC_fgsea.csv. For visualisation, it reloads a filtered subset of terms (those flagged plot == 1), assigns significance stars based on adjusted p-values, orders pathways by NES, and produces a horizontal bar plot of NES coloured by direction (negative NES = “Up P14”, positive NES = “Up P15”), saving the figure as figures/fgsea_P15vP14_HYP.pdf.

**Multiple_t-tests**
This script loads a CSV of numeric data, cleans the column names, coerces values to numbers, drops rows with missing data, and then runs pairwise comparisons in column pairs (column 1 vs 2, 3 vs 4, etc.). It first checks whether the data are approximately normal using Shapiro–Wilk tests (requiring ≥3 values per column) and then chooses tests accordingly: if normal, it uses a paired t-test (within-subject) or unpaired t-test (between-subject); if not normal, it uses Wilcoxon signed-rank (paired) or Mann–Whitney U (unpaired). For unpaired parametric comparisons it also uses Tukey’s HSD to obtain an adjusted p-value for that specific pair, and for unpaired nonparametric comparisons it attempts Dunn’s test with Benjamini–Hochberg FDR to get an adjusted p-value; for paired tests it collects raw p-values across all pairs and then applies a single Benjamini–Hochberg FDR correction across the set of paired comparisons. Finally, it assigns significance stars based on the (corrected) p-values and exports a results table (groups compared, whether parametric, test used, posthoc/correction, raw and corrected p-values, and stars) to a “_results.csv” file.

**PCA_Microglia**
This script loads a single CSV dataset, drops rows with missing values, separates the first column as an ID field, and then prepares the remaining numeric features for dimensionality reduction and clustering. It adds simple metadata (source file, ID, and a “Cluster” label based on the first two characters of the ID), standardizes all numeric features with StandardScaler, and runs a 2-component PCA to summarise the main axes of variance, printing the PCA loadings (feature contributions) and the percent variance explained by PC1 and PC2. It then performs K-means clustering (k=4) on the scaled feature space and appends the resulting cluster labels to the dataset, and in parallel computes a 2D UMAP embedding of the same scaled data for visualization. All metadata, K-means labels, PCA coordinates, and UMAP coordinates are concatenated into a final table that is saved to a new CSV; it also computes the mean UMAP position (center) of each K-means cluster, calculates pairwise Euclidean distances between these cluster centers, prints them, and finally plots the UMAP scatter coloured by K-means cluster with cluster centers overlaid as black “X” markers.

**PCA_Microglia_Prep**
This script batch-builds a single “master” table by scanning one or more user-specified directories for a set of Imaris-style CSV outputs that share a common ID, extracting only the columns you care about from each file, and merging them together. For each filename pattern listed in file_columns (e.g., _Filament_Length_(sum).csv, _Soma_Volume.csv), it looks for the first file in the directory that ends with that pattern, reads it while skipping the first three header rows, keeps only the specified ID + measurement columns, and standardizes the join key by renaming FilamentID to ID when present. It then outer-merges each new mini-table into a global merged_df on ID so that all measurements accumulate into one wide dataframe (preserving IDs even if some files are missing values), finally reorders columns to put ID first and writes the merged result to FILENAME_cleaned.csv.

**PCA_Spines**
This script loads a merged morphometrics CSV, selects a defined set of spine feature columns, drops rows with missing values in those features, and standardizes them before running K-means clustering (k=4) to assign each spine to a cluster. It then computes 2D PCA (saving the PCA coordinates back into the dataframe and exporting the PCA loadings to a separate CSV) and 2D UMAP to generate low-dimensional embeddings for visualization/interpretation. Independently of clustering, it assigns each spine a rule-based “Spine Type” (Stubby/Filopodia/Mushroom/Thin/Other) based on total spine-part length and the relationship between head vs neck diameter. The updated dataframe (with Cluster, PCA1/2, UMAP1/2, and Spine Type) is saved to a new CSV, and the script also produces several summary tables: counts of each spine type per FilamentID (and source file), counts of each K-means cluster per FilamentID (and source file), an updated per-filament table combining spine-type counts with filament length and cluster counts, and a contingency-style table counting how often each cluster occurs within each spine type, exporting each of these summaries as separate CSV files.

**PCA_withPermutationTesting**
This script loads and concatenates all CSV files in a target directory, treating the first column of each file as an ID, adding metadata for the source filename, and assigning a coarse “Cluster” label based on the first two characters of each ID. It then performs a 2D PCA on the feature matrix (excluding metadata), uses the PCA scores as input to a 2D UMAP embedding (with n_neighbors capped at 15 or N−1), and combines the original data, PCA coordinates, and UMAP coordinates into a single results table that is saved to disk. To quantify how separated the ID-defined clusters are in PCA space, it computes each cluster’s PCA centroid, prints the centroid coordinates, and calculates the pairwise Euclidean distances between cluster centroids. It then runs a permutation test (1000 permutations) by independently shuffling each feature column across samples, projecting the shuffled data into the original PCA space, recomputing cluster centroids, and building null distributions of centroid-to-centroid distances for every cluster pair; p-values are computed as the fraction of shuffled distances that are greater than or equal to the observed distance, and the mean ± SEM of the shuffled distances are reported. Finally, it exports the null distributions to a CSV, saves the full annotated dataset, plots the UMAP embedding coloured by cluster, and for each cluster pair plots the shuffled distance histogram with the observed distance overlaid as a reference line.

**Photometry**
This script preprocesses dual-channel fibre photometry data to produce a detrended, motion-corrected fluorescence signal. It loads raw 470 nm (Ca-dependent) and 415 nm (isosbestic/control) recordings for two fibres from CSVs (columns 8 and 9), loads separate 470 nm background recordings to estimate a baseline background level per fibre (median), and constructs a time vector in seconds from the camera metadata assuming 30 fps. It then clips all photometry values to a maximum of 1, subtracts the 470 nm background from each fibre’s 470 trace to get a background-corrected calcium channel, and keeps the 415 traces as the isosbestic reference. To model slow drift/photobleaching in the isosbestic channel, it fits a bi-exponential decay to each 415 trace, then linearly regresses the fitted isosbestic signal onto the calcium channel to scale and offset it so it matches the calcium trace’s baseline fluctuations. This scaled fit is treated as the estimated baseline/artifact component; subtracting it from the calcium channel yields a detrended fluorescence signal, which is shifted upward by adding the minimum value (to keep signals positive/consistent). Finally, it exports the processed fibre 1 signal to an Excel file (with fibre 2 export optional) and prints “DONE” when finished.

**QuPath_CellCounts**
This script loads a Cellfinder/QuPath summary CSV for a single subject, removes a predefined set of unwanted structures (largely ventricles, fibre tracts, broad parent regions, and other non-target areas), and then standardises and simplifies the remaining region labels to produce a cleaner region-level cell-density table. It first filters out any rows whose structure_name matches the exclusion list, then cleans region names by replacing “/” with “,” and creates a simplified label by taking only the first comma-delimited part of each structure name so related subregions collapse into a common parent label. For regions in a specified ROI list, it groups by this simplified label and sums counts/volumes to get one entry per region, while leaving other regions as their original names. It then computes Cells per mm³ as total_cells / total_volume_mm3, replaces any infinite values with zero, and additionally collapses several medial preoptic subregions into a single combined “MPOA” row by summing their values (if an MPOA row doesn’t already exist). Finally, it sorts regions by cell density, keeps only the key output columns (region name, total cells, total volume, and cells/mm³), and exports the cleaned table to an Excel file alongside printing it to the console.
