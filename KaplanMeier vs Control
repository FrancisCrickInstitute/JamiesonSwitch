import pandas as pd
import os
import numpy as np
from lifelines import KaplanMeierFitter
from lifelines.statistics import logrank_test
import warnings
from statsmodels.stats.multitest import multipletests  # For Benjamini-Hochberg FDR

# Suppress warnings
warnings.simplefilter(action='ignore', category=FutureWarning)

# File paths
file_path = 'FILEPATH.csv'  # Update with the actual file path

# Load data
retr = pd.read_csv(file_path)

# Reshape data for Kaplan-Meier analysis
# Create a long format DataFrame where each row represents an individual observation
df_long = pd.melt(retr, id_vars=[], var_name='Timepoint', value_name='Retrieval')

# Create the event column where retrieval < 600 is considered an event
df_long['Event'] = np.where(df_long['Retrieval'] < 600, 1, 0)

# Sort by timepoint to ensure proper ordering
df_long = df_long.sort_values(by='Timepoint')

# Create a list to store log-rank test results
logrank_results = []

# Define a function to perform Kaplan-Meier fitting and log-rank test
def perform_logrank_test(T1, E1, T2, E2, label1, label2):
    results = logrank_test(T1, T2, event_observed_A=E1, event_observed_B=E2)
    
    # Add results to the list
    raw_p_value = results.p_value
    results_list.append({
        'Group 1': label1,
        'Group 2': label2,
        'Test': 'Kaplan-Meyer survival analysis log-rank test',
        'Post-hoc test': 'Benjamini-Hochberg False Discovery Rate',  # Placeholder
        'Raw p-value': raw_p_value,
        'Corrected p-value': None,  # Placeholder for now
        'Significance': significance_stars(raw_p_value)
    })
    
    print(f"{label1} vs {label2}:")
    results.print_summary()
    print(f"p-value: {results.p_value}")
    print(f"Test Statistic: {results.test_statistic}")

# Define significance stars based on p-value
def significance_stars(p_value):
    """Assign stars based on p-value thresholds."""
    if p_value < 0.0001:
        return "****"
    elif p_value < 0.001:
        return "***"
    elif p_value < 0.01:
        return "**"
    elif p_value < 0.05:
        return "*"
    else:
        return ""

# Predefined control group (Vir)
control_column = 'Vir.'

# Get unique timepoints (excluding 'Vir.')
timepoints = [col for col in retr.columns if col != control_column]

# Initialize list to store results
results_list = []

# Run comparisons: compare each timepoint (excluding 'Vir') with 'Vir'
for timepoint in timepoints:
    T1 = df_long[df_long['Timepoint'] == control_column]['Retrieval']  # Control group (Vir)
    E1 = df_long[df_long['Timepoint'] == control_column]['Event']  # Event column for control group
    T2 = df_long[df_long['Timepoint'] == timepoint]['Retrieval']  # Other timepoint
    E2 = df_long[df_long['Timepoint'] == timepoint]['Event']  # Event column for other timepoint
    perform_logrank_test(T1, E1, T2, E2, control_column, timepoint)

# Convert results list to DataFrame
results_df = pd.DataFrame(results_list)

# Extract the raw p-values for correction
raw_p_values = results_df['Raw p-value'].values

# Apply Benjamini-Hochberg FDR correction
rejected, corrected_p_values, _, _ = multipletests(raw_p_values, method='fdr_bh')

# Update the corrected p-values in the results DataFrame
results_df['Corrected p-value'] = corrected_p_values

# Add significance stars for corrected p-values
results_df['Significance'] = results_df['Corrected p-value'].apply(significance_stars)

# Save log-rank results to CSV
output_file = file_path.replace(".csv", "_results.csv")
results_df.to_csv(output_file, index=False)

print(f"\nResults saved to: {output_file}")
